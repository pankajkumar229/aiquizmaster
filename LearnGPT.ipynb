{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load variables from .env file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\") \n",
    "loader = PyPDFLoader(\"./DMV_Handbook.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# select which embeddings we want to use\n",
    "embeddings = OpenAIEmbeddings(openai_api_key = api_key)\n",
    "\n",
    "# create the vectorestore to use as the index\n",
    "db = Chroma.from_documents(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credits : Inspired by Kimchoi, https://github.com/kimchoi-jjiggae/quizmegpt\n",
    "\n",
    "def generate_questions(doc):\n",
    "    # Construct the prompt for the GPT model\n",
    "    prompt = f\"Generate 1 factual test question whose answer should be a single word, based on this text, and do not include the answer: {doc}\"  # noqa: E501\n",
    "    \n",
    "    # Call the OpenAI API for chat completion\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a teacher. I need you to help write me exam questions.\",  # noqa: E501\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=1200,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extract the generated HTML from the response\n",
    "    reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return reply\n",
    "\n",
    "\n",
    "def evaluate_questions(query, result, user_answer):\n",
    "    # Construct the prompt for the GPT model\n",
    "    prompt = f\"give encouraging feedback to this student, telling them if they are correct or wrong and how to improve if applicable: {query} 'correct answer': ' {result['result']}', student answer: {user_answer}\"  # noqa: E501\n",
    "\n",
    "    # Call the OpenAI API for chat completion\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a teacher. I need you to help grade exams.\",  # noqa: E501\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=1200,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extract the generated HTML from the response\n",
    "    reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return reply\n",
    "\n",
    "\n",
    "def generate_followup_questions(doc, query, result, user1_answer, user2_answer):\n",
    "    # Construct the prompt for the GPT model\n",
    "    prompt = f\"Here is the question and student answers. If both students answered correctly, please suggest another question. If one student got it correct but the other didn't please generate a follow up question so that one peer can teach another peer, ask another factual test question whose answer should be a single word based on this text. If both got it wrong, ask the same question again. 'Document': ' {doc} ', 'question': ' {query} ', 'correct answer': ' {result['result']}', student 1 answer: {user1_answer}, student 2 answer: {user2_answer}\"  # noqa: E501\n",
    "    \n",
    "    # Call the OpenAI API for chat completion\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a teacher. I need you to create a collaborative exercise so peers can learn from each other.\",  # noqa: E501\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=1200,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extract the generated HTML from the response\n",
    "    reply = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return reply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "\n",
      "\n",
      "GENERATED QUESTION: What does a single solid yellow line on the center of a road with two-way traffic indicate? \n",
      "\n",
      "\n",
      "********************\n",
      "\n",
      "\n",
      "********* Generating Feedback ***********\n",
      "\n",
      "FEEDBACK Student 1: Dear student, thank you for submitting your answer. Unfortunately, your answer does not address the question. The correct answer is: \"A single solid yellow line indicates that you should not pass a vehicle in front of you.\" It is important to pay attention to road markings as they provide valuable information for safe driving. Keep practicing and paying attention to details. You'll get there! \n",
      "\n",
      "\n",
      "FEEDBACK Student 2: Dear student, thank you for taking the time to answer this question. Unfortunately, your response 'fkshfs' is not the correct answer. The correct answer is 'A single solid yellow line indicates that you should not pass a vehicle in front of you.' Please take some time to review this information and remember to read questions carefully before answering. Keep up the good work! \n",
      "\n",
      "\n",
      "********* Complete Answer ***********\n",
      "\n",
      "CORRECT ANSWER:  A single solid yellow line indicates that you should not pass a vehicle in front of you.\n",
      "\n",
      "********* Generating Follow-up collaboration ***********\n",
      "\n",
      "GENERATED Followup: Let's ask the same question again: What does a single solid yellow line on the center of a road with two-way traffic indicate? Please answer with a single word. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********* Keep Going! ***********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expose this index in a retriever interface\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# create a chain to answer questions \n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "# print(\"TEXT\", texts[5])\n",
    "query = generate_questions(texts[20])\n",
    "\n",
    "print ('********************\\n\\n')\n",
    "print('GENERATED QUESTION:', query, '\\n\\n')\n",
    "result = qa({\"query\": query})\n",
    "print ('********************\\n')\n",
    "user1_answer = input(\"Enter your answer student 1: \")\n",
    "user2_answer = input(\"Enter your answer student 2: \")\n",
    "\n",
    "print ('\\n********* Generating Feedback ***********\\n')\n",
    "feedback1 = evaluate_questions(query, result, user1_answer)\n",
    "print('FEEDBACK Student 1:', feedback1, '\\n\\n')\n",
    "feedback2 = evaluate_questions(query, result, user2_answer)\n",
    "print('FEEDBACK Student 2:', feedback2, '\\n\\n')\n",
    "\n",
    "print ('********* Complete Answer ***********\\n')  \n",
    "print('CORRECT ANSWER:', result['result'])\n",
    "\n",
    "print ('\\n********* Generating Follow-up collaboration ***********\\n')\n",
    "followup = generate_followup_questions(texts[20], query, result, user1_answer, user2_answer)\n",
    "print('GENERATED Followup:', followup, '\\n\\n')\n",
    "\n",
    "print ('\\n\\n\\n********* Keep Going! ***********\\n')  \n",
    "\n",
    "\n",
    "# print ('********************\\n')\n",
    "# user1_answer = input(\"Enter your answer student 1: \")\n",
    "# user2_answer = input(\"Enter your answer student 2: \")\n",
    "\n",
    "\n",
    "# print ('\\n********* Generating Feedback ***********\\n')\n",
    "# feedback1 = evaluate_questions(query, result, user1_answer)\n",
    "# print('FEEDBACK Student 1:', feedback1, '\\n\\n')\n",
    "# feedback2 = evaluate_questions(query, result, user2_answer)\n",
    "# print('FEEDBACK Student 2:', feedback2, '\\n\\n')\n",
    "\n",
    "\n",
    "# print ('\\n********* Generating Follow-up collaboration ***********\\n')\n",
    "# followup = generate_followup_questions(texts[20], query, result, user1_answer, user2_answer)\n",
    "# print('GENERATED Followup:', followup, '\\n\\n')\n",
    "\n",
    "# print ('\\n\\n\\n********* Keep Going! ***********\\n')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03be76d704657592f0da3f8d7d4a43778a84fe027d01be60480387902d249023"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
